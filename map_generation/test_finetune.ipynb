{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler, StableDiffusionInpaintPipeline\n",
    "from osm_dataset import TextToImageDataset\n",
    "from config import CHECKPOINTS_DIR, MODEL_NAME, DEVICE, DATA_DIR\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prediction(pipe: StableDiffusionPipeline, prompt: str):\n",
    "    with torch.no_grad():\n",
    "        images = pipe(prompt, height=256, width=256).images\n",
    "\n",
    "    return images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !python diff_hugging.py\n",
    "# import diff_hugging\n",
    "# diff_hugging.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/Documents/mgr/semestr_II/nlp/nlp_l2/nlp_venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "The config attributes {'dropout': 0.0} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        \"models/wroclaw_v2\", subfolder=\"scheduler\"\n",
    "    )\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"models/wroclaw_v2\", safety_checker=None, scheduler=noise_scheduler).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f219ea19db284eccb298505306d52e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/marcin/.cache/huggingface/datasets/imagefolder/Wrocław, PL-16adbde1af182d57/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce0e390c68b42b89c6c5c082bcd0b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4367a99be304a87987d20ed56ece06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = TextToImageDataset(DATA_DIR)\n",
    "dl = DataLoader(ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(dl, pipeline):\n",
    "    total_loss = 0.0\n",
    "    for img, caption in tqdm(dl):\n",
    "        latents = pipeline.vae.encode(\n",
    "                        img.to(DEVICE)\n",
    "                    ).latent_dist.sample()\n",
    "        latents = latents * pipeline.vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz,),\n",
    "            device=latents.device,\n",
    "        )\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = pipeline.text_encoder(caption.to(DEVICE))[0]\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "            )\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        model_pred = pipeline.unet(\n",
    "            noisy_latents, timesteps, encoder_hidden_states\n",
    "        ).sample\n",
    "\n",
    "        \n",
    "        loss = F.mse_loss(\n",
    "            model_pred.float(), target.float(), reduction=\"mean\"\n",
    "            )\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2144/2144 [03:25<00:00, 10.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12338643512209016"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dl, pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "685727852e42551632e0fc1e43e361d886b7ddaddaf1474b2acdf6903161a758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
