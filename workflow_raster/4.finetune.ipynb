{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from map_generation.osm_dataset import TextToImageDataset\n",
    "from config import DEVICE, DATA_DIR\n",
    "from torch.utils.data import DataLoader\n",
    "import map_generation.diff_hugging as diff_hugging\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dac40d3b1e548a5a46b236164661a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/32.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/marcin/.cache/huggingface/datasets/mprzymus___parquet/mprzymus--osm_tiles_small-199229e0bb266c92/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec6ac87ed3b45c7b6ac491209aab1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070db8c9258f46a88be8a325337c2d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/34.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147577a2d74a4d44947175e004e1ff55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/marcin/.cache/huggingface/datasets/mprzymus___parquet/mprzymus--osm_tiles_small-199229e0bb266c92/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef22cbfea7d4538acef9bf8ea931f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'predict_epsilon': True} were passed to DDPMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/marcin/Documents/mgr/projekt_n_w/own/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: logs/testing_workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                 | Params\n",
      "------------------------------------------------------\n",
      "0 | text_encoder | CLIPTextModel        | 123 M \n",
      "1 | vae          | AutoencoderKL        | 83.7 M\n",
      "2 | unet         | UNet2DConditionModel | 579 M \n",
      "------------------------------------------------------\n",
      "579 M     Trainable params\n",
      "206 M     Non-trainable params\n",
      "786 M     Total params\n",
      "3,144.397 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c73c6da7118407783563b4548495f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/Documents/mgr/projekt_n_w/own/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:135: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Epoch 0, global step 2144: 'train/loss' reached 0.13991 (best 0.13991), saving model to '/home/marcin/Documents/mgr/projekt_n_w/own/workflow_raster/model/testing_workflow.ckpt' as top 1\n",
      "Epoch 1, global step 4288: 'train/loss' reached 0.12888 (best 0.12888), saving model to '/home/marcin/Documents/mgr/projekt_n_w/own/workflow_raster/model/testing_workflow.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'predict_epsilon': True} were passed to DDPMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "The config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "diff_hugging.main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing if loss is reasonable\n",
    "\n",
    "To make sure saving works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(dl, pipeline, DEVICE=DEVICE):\n",
    "    total_loss = 0.0\n",
    "    for img, caption in tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            latents = pipeline.vae.encode(\n",
    "                            img.to(DEVICE)\n",
    "                        ).latent_dist.sample()\n",
    "            latents = latents * pipeline.vae.config.scaling_factor\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=latents.device,\n",
    "            )\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = pipeline.text_encoder(caption.to(DEVICE))[0]\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                )\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            model_pred = pipeline.unet(\n",
    "                noisy_latents, timesteps, encoder_hidden_states\n",
    "            ).sample\n",
    "\n",
    "        \n",
    "        loss = F.mse_loss(\n",
    "            model_pred.float(), target.float(), reduction=\"mean\"\n",
    "            )\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/marcin/.cache/huggingface/datasets/mprzymus___parquet/mprzymus--osm_tiles_small-199229e0bb266c92/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a6bc044a74ba8876fd1b5ffa80257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/marcin/.cache/huggingface/datasets/mprzymus___parquet/mprzymus--osm_tiles_small-199229e0bb266c92/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b7cef2b1443476bf.arrow\n",
      "The config attributes {'predict_epsilon': True} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "The config attributes {'predict_epsilon': True} were passed to DDPMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "100%|██████████| 2144/2144 [04:13<00:00,  8.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13314637700256496"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = TextToImageDataset(DATA_DIR)\n",
    "dl = DataLoader(ds, batch_size=1)\n",
    "model_name = f\"{config.CHECKPOINTS_DIR}/{config.MODEL_NAME}_pipeline\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_name)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        model_name, subfolder=\"scheduler\"\n",
    "    )\n",
    "evaluate(dl, pipeline.to(DEVICE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5dce9f45505a373811135736fcc1a28d7cb642c7707cc61eb0340150084562f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
